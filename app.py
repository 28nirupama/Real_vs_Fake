from fastapi import FastAPI, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse
import joblib
import numpy as np
from extra_features import ExtraFeatures
from scipy.sparse import hstack as sparse_hstack

app = FastAPI()

# Allow frontend access
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],        # You can restrict later
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Serve static files
app.mount("/static", StaticFiles(directory="static"), name="static")

@app.get("/")
async def home():
    return FileResponse("index.html")


# -------- Load ML Model + Vectorizer ----------
try:
    model = joblib.load("ai_human_model.pkl")
    vectorizer = joblib.load("vectorizer.pkl")
    extra = ExtraFeatures()
    print("âœ… Model & Vectorizer Loaded Successfully")
except Exception as e:
    print("âŒ Error loading model:", e)


# Funny responses
funny_responses = {
    "human": [
        "Detected: HUMAN ğŸ¤¦, typos = proof of existence!",
        "Yep, definitely human. The chaos is real ğŸ˜‚",
        "Human detected, certified emotional creature ğŸ¥²",
        "100% human! The grammar struggles gave it away ğŸ˜­",
        "Human spotted â€” brain lag detected ğŸ§ ğŸ’¤",
        "Looks humanâ€¦ messy, unpredictable, totally normal ğŸ˜Œ",
        "This text screams â€˜I typed this half asleepâ€™ ğŸ˜ª",
        "Human vibes detected â€” emotions everywhere ğŸ˜­â¤ï¸",
        "This is so human it probably needs coffee â˜•",
        "Human confirmed â€” proudly imperfect since forever ğŸ˜…"
    ],
    "ai": [
        "AI detected ğŸ¤–, too smooth to be human!",
        "This text smells like silicon chips and algorithms ğŸ˜",
        "AI spotted, no typos, suspiciously perfect grammar ğŸ˜‚",
        "Definitely AI â€” humans donâ€™t write this clean ğŸ˜²",
        "This is so polished it has to be a robot ğŸ§½ğŸ¤–",
        "AI confirmed â€” zero drama, zero emotions ğŸ˜Œ",
        "This text is 100% machine â€” even my circuits are impressed âš™ï¸",
        "AI detected â€” looks like it was generated at 0.0001 seconds âš¡",
        "Robot vibes everywhereâ€¦ beep boop ğŸ¤–âœ¨",
        "AI alert! Too logical, too structured, too perfect ğŸ˜†"
    ]
}


# -------- Prediction Endpoint ----------
@app.post("/predict")
async def predict(text: str = Form(...)):
    try:
        clean_text = text.strip()

        # Vectorize + extra features
        text_vec = vectorizer.transform([clean_text])
        extra_vec = extra.transform([clean_text])

        final_features = sparse_hstack([text_vec, extra_vec])

        prediction = model.predict(final_features)[0]
        funny = np.random.choice(funny_responses.get(prediction, ["No clue ğŸ˜­"]))

        return {
            "prediction": prediction,
            "funny_response": funny
        }

    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": f"Backend error: {str(e)}"}
        )
