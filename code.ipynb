{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6eb74a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 1Ô∏è‚É£ Import required libraries\n",
    "# ================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.utils import resample\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c6aa44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 2Ô∏è‚É£ Extra Features Class\n",
    "# ================================\n",
    "# This is same as in your extra_features.py\n",
    "\n",
    "emoji_set = set(\n",
    "    \"üòÄüòÅüòÇü§£üòÉüòÑüòÖüòÜüòâüòäüòãüòéüòçüòòüòóüòôüòöüôÇü§óü§©ü§îü§®üòêüòëüò∂üôÑüòèüò£üò•\"\n",
    "    \"üòÆü§êüòØüò™üò´üò¥üòåü§ìüòõüòúüòùü§§üòíüòìüòîüòïüôÉü§ëüò≤‚òπÔ∏èüôÅüòñüòûüòüüò§üò¢üò≠\"\n",
    "    \"üò¶üòßüò®üò©ü§Øüò¨üò∞üò±üò≥ü§™üòµüò°üò†ü§¨\"\n",
    ")\n",
    "\n",
    "class ExtraFeatures(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, texts):\n",
    "        processed = [t if isinstance(t, str) else \"\" for t in texts]\n",
    "\n",
    "        emoji_count = np.array([sum(1 for ch in t if ch in emoji_set) for t in processed]).reshape(-1, 1)\n",
    "        punctuation_ratio = np.array([sum(1 for c in t if c in string.punctuation) / (len(t) + 1) for t in processed]).reshape(-1, 1)\n",
    "        digit_ratio = np.array([sum(1 for c in t if c.isdigit()) / (len(t) + 1) for t in processed]).reshape(-1, 1)\n",
    "        avg_word_len = np.array([\n",
    "            np.mean([len(w) for w in t.split()]) if len(t.split()) > 0 else 0\n",
    "            for t in processed\n",
    "        ]).reshape(-1, 1)\n",
    "\n",
    "        return np.hstack([emoji_count, punctuation_ratio, digit_ratio, avg_word_len])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5d3dd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 1218\n",
      "label\n",
      "human    609\n",
      "ai       609\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 3Ô∏è‚É£ Load your CSV data\n",
    "# ================================\n",
    "df = pd.read_csv(\"RF_data.csv\")  # Your CSV with Human_Content & AI_Content columns\n",
    "\n",
    "# Combine into a single dataframe\n",
    "human_df = pd.DataFrame({\"text\": df[\"Human_Content\"], \"label\": \"human\"})\n",
    "ai_df = pd.DataFrame({\"text\": df[\"AI_Content\"], \"label\": \"ai\"})\n",
    "df_final = pd.concat([human_df, ai_df], ignore_index=True)\n",
    "\n",
    "# Clean text\n",
    "df_final['text'] = df_final['text'].fillna(\"\").str.strip()\n",
    "\n",
    "print(\"Total samples:\", df_final.shape[0])\n",
    "print(df_final['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f14c7823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced samples: (1218, 2)\n",
      "label\n",
      "human    609\n",
      "ai       609\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 4Ô∏è‚É£ Balance the classes (oversampling)\n",
    "# ================================\n",
    "human = df_final[df_final.label == \"human\"]\n",
    "ai = df_final[df_final.label == \"ai\"]\n",
    "\n",
    "if len(human) > len(ai):\n",
    "    ai = resample(ai, replace=True, n_samples=len(human), random_state=42)\n",
    "elif len(ai) > len(human):\n",
    "    human = resample(human, replace=True, n_samples=len(ai), random_state=42)\n",
    "\n",
    "df_balanced = pd.concat([human, ai]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(\"Balanced samples:\", df_balanced.shape)\n",
    "print(df_balanced['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f3d218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  label  \\\n",
      "0    If you believe the home alarm commercials, the...  human   \n",
      "1    What do you call a midget with no teeth A gum ...  human   \n",
      "2    It's just sad how often I see zookeepers break...  human   \n",
      "3    You really have to question the judgment of pe...  human   \n",
      "4    What's green and smells like bacon? Kermit the...  human   \n",
      "..                                                 ...    ...   \n",
      "395  The correct volume level for contemplating com...     ai   \n",
      "396  It is a historical certainty that all forgotte...     ai   \n",
      "397  The only thing preventing the world from achie...     ai   \n",
      "398  The phenomenon of deja vu is a momentary serve...     ai   \n",
      "399  The structural formula for a truly unproductiv...     ai   \n",
      "\n",
      "                                            clean_text  \n",
      "0    ifyoubelievethehomealarmcommercialsthefirstthi...  \n",
      "1               whatdoyoucallamidgetwithnoteethagumjob  \n",
      "2    itsjustsadhowofteniseezookeepersbreakingtheiro...  \n",
      "3    youreallyhavetoquestionthejudgmentofpeoplewhoh...  \n",
      "4     whatsgreenandsmellslikebaconkermitthefrogsfinger  \n",
      "..                                                 ...  \n",
      "395  thecorrectvolumelevelforcontemplatingcomplexfi...  \n",
      "396  itisahistoricalcertaintythatallforgottenidease...  \n",
      "397  theonlythingpreventingtheworldfromachievingper...  \n",
      "398  thephenomenonofdejavuisamomentaryserverlaginth...  \n",
      "399  thestructuralformulaforatrulyunproductivemeeti...  \n",
      "\n",
      "[400 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 5Ô∏è‚É£ Train-Test Split\n",
    "# ================================\n",
    "X = df_balanced[\"text\"]\n",
    "y = df_balanced[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.10, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Training samples:\", X_train.shape[0])\n",
    "print(\"Testing samples:\", X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d11e166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 1096\n",
      "Testing samples: 122\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 5Ô∏è‚É£ Train-Test Split\n",
    "# ================================\n",
    "X = df_balanced[\"text\"]\n",
    "y = df_balanced[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.10, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Training samples:\", X_train.shape[0])\n",
    "print(\"Testing samples:\", X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4bdd3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 6Ô∏è‚É£ Vectorization + Extra Features\n",
    "# ================================\n",
    "# TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=15000, ngram_range=(1,3), sublinear_tf=True)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Extra features\n",
    "extra = ExtraFeatures()\n",
    "X_train_extra = extra.fit_transform(X_train)\n",
    "X_test_extra = extra.transform(X_test)\n",
    "\n",
    "# Combine\n",
    "X_train_combined = hstack([X_train_tfidf, X_train_extra])\n",
    "X_test_combined = hstack([X_test_tfidf, X_test_extra])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d479ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ================================\n",
    "# 7Ô∏è‚É£ Train the Linear SVC Model\n",
    "# ================================\n",
    "model = LinearSVC()\n",
    "model.fit(X_train_combined, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eebedfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 83.61%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          ai       0.92      0.74      0.82        61\n",
      "       human       0.78      0.93      0.85        61\n",
      "\n",
      "    accuracy                           0.84       122\n",
      "   macro avg       0.85      0.84      0.83       122\n",
      "weighted avg       0.85      0.84      0.83       122\n",
      "\n",
      "Confusion Matrix:\n",
      "[[45 16]\n",
      " [ 4 57]]\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 8Ô∏è‚É£ Evaluate Model\n",
    "# ================================\n",
    "y_pred = model.predict(X_test_combined)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy*100:.2f}%\\n\")\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e201a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I had a great day at the park with friends!... Prediction: ai\n",
      "Text: The probability distribution of X is calculated us... Prediction: ai\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 9Ô∏è‚É£ Test on new sentences\n",
    "# ================================\n",
    "def predict_text(text):\n",
    "    vec = tfidf.transform([text])\n",
    "    extra_feat = extra.transform([text])\n",
    "    combined = hstack([vec, extra_feat])\n",
    "    pred = model.predict(combined)[0]\n",
    "    return pred\n",
    "\n",
    "# Example sentences\n",
    "examples = [\n",
    "    \"I had a great day at the park with friends!\",\n",
    "    \"The probability distribution of X is calculated using the formula...\"\n",
    "]\n",
    "\n",
    "for s in examples:\n",
    "    print(f\"Text: {s[:50]}... Prediction: {predict_text(s)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61ad516c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and vectorizer saved.\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 10Ô∏è‚É£ Save model & vectorizer\n",
    "# ================================\n",
    "joblib.dump(model, \"ai_human_model.pkl\")\n",
    "joblib.dump(tfidf, \"vectorizer.pkl\")\n",
    "print(\"Model and vectorizer saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49878e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_sentences = [\n",
    "    \"I had a fantastic weekend hiking with my friends!\",\n",
    "    \"Can't believe how much I laughed at that movie last night.\",\n",
    "    \"My dog loves playing fetch in the park every morning.\",\n",
    "    \"I baked a chocolate cake yesterday and it turned out amazing!\",\n",
    "    \"I feel so tired today, I think I need a nap.\"\n",
    "]\n",
    "ai_sentences = [\n",
    "    \"The probability of X is calculated by applying Bayes theorem and integrating over the sample space.\",\n",
    "    \"Machine learning models require training datasets with labeled examples to optimize the loss function efficiently.\",\n",
    "    \"The algorithm demonstrates a time complexity of O(n log n) under the assumption of a balanced binary search tree.\",\n",
    "    \"Quantum computing utilizes qubits which can exist in superposition states to perform parallel computations.\",\n",
    "    \"The economic growth rate is influenced by multiple macroeconomic indicators including inflation, unemployment, and interest rates.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7260ace4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Human Sentences ===\n",
      "Text: I had a fantastic weekend hiking with my friends!... Prediction: human\n",
      "Text: Can't believe how much I laughed at that movie las... Prediction: human\n",
      "Text: My dog loves playing fetch in the park every morni... Prediction: ai\n",
      "Text: I baked a chocolate cake yesterday and it turned o... Prediction: human\n",
      "Text: I feel so tired today, I think I need a nap.... Prediction: human\n",
      "\n",
      "=== AI Sentences ===\n",
      "Text: The probability of X is calculated by applying Bay... Prediction: ai\n",
      "Text: Machine learning models require training datasets ... Prediction: ai\n",
      "Text: The algorithm demonstrates a time complexity of O(... Prediction: ai\n",
      "Text: Quantum computing utilizes qubits which can exist ... Prediction: ai\n",
      "Text: The economic growth rate is influenced by multiple... Prediction: ai\n"
     ]
    }
   ],
   "source": [
    "# Function to predict\n",
    "def predict_text(text):\n",
    "    vec = tfidf.transform([text])\n",
    "    extra_feat = extra.transform([text])\n",
    "    combined = hstack([vec, extra_feat])\n",
    "    pred = model.predict(combined)[0]\n",
    "    return pred\n",
    "\n",
    "# Test human sentences\n",
    "print(\"=== Human Sentences ===\")\n",
    "for s in human_sentences:\n",
    "    print(f\"Text: {s[:50]}... Prediction: {predict_text(s)}\")\n",
    "\n",
    "# Test AI sentences\n",
    "print(\"\\n=== AI Sentences ===\")\n",
    "for s in ai_sentences:\n",
    "    print(f\"Text: {s[:50]}... Prediction: {predict_text(s)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cc7669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "Bro today I lost my charger üò≠ -> Human-Written (score: 0.496)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "Neural networks consist of layers of interconnected nodes. -> Human-Written (score: 0.496)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "I swear my phone hates me bro -> Human-Written (score: 0.496)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "Machine learning models require structured datasets -> Human-Written (score: 0.496)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88601f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples after cleaning: 400\n",
      "Human samples (0): 200\n",
      "AI samples (1): 200\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# --- 1. Data Loading and Preprocessing (Using User's Logic + Fixes) ---\n",
    "# NOTE: Ensure 'AIdata.csv' contains ALL your 400 samples (200 Human_Content, 200 AI_Content).\n",
    "try:\n",
    "    df = pd.read_csv('RF data.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'AIdata.csv' not found. Please ensure your combined dataset is in the working directory.\")\n",
    "    # Exit the script gracefully if the file is missing\n",
    "    X, y = [], []\n",
    "    exit()\n",
    "\n",
    "# 1. Separate and Label Human Content\n",
    "# FIX: Mapping 'Human' content to numerical label 0\n",
    "df_human = pd.DataFrame({\n",
    "    \"text\": df[\"Human_Content\"],\n",
    "    \"label\": 0 \n",
    "})\n",
    "\n",
    "# 2. Separate and Label AI Content\n",
    "# FIX: Mapping 'AI' content to numerical label 1\n",
    "df_ai = pd.DataFrame({\n",
    "    \"text\": df[\"AI_Content\"],\n",
    "    \"label\": 1\n",
    "})\n",
    "\n",
    "# 3. Combine DataFrames and Clean\n",
    "df_final = pd.concat([df_human, df_ai], ignore_index=True)\n",
    "\n",
    "\n",
    "# Apply dropna to clean up any missing text entries after concatenation\n",
    "df_final = df_final.dropna(subset=['text'])\n",
    "\n",
    "print(f\"Total samples after cleaning: {len(df_final)}\")\n",
    "print(f\"Human samples (0): {df_final['label'].value_counts().get(0, 0)}\")\n",
    "print(f\"AI samples (1): {df_final['label'].value_counts().get(1, 0)}\")\n",
    "\n",
    "X = df_final['text']\n",
    "y = df_final['label']\n",
    "\n",
    "# Check if there's enough data to proceed\n",
    "if len(df_final) < 2:\n",
    "    print(\"\\nError: Not enough samples remaining after cleaning to train the model.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Feature Extraction ---\n",
    "# Using TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')\n",
    "X_vec = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets (using stratification to maintain label proportions)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_vec, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# --- 3. Model Training with Class Weight Balancing ---\n",
    "# FIX: class_weight='balanced' parameter to counteract data imbalance\n",
    "# This forces the model to heavily penalize errors on the less-represented class (Human/Label 0),\n",
    "# directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27effece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Logistic Regression with class_weight='balanced' ---\n",
      "\n",
      "Classification Report (Balanced Model):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Human       1.00      0.97      0.99        40\n",
      "          AI       0.98      1.00      0.99        40\n",
      "\n",
      "    accuracy                           0.99        80\n",
      "   macro avg       0.99      0.99      0.99        80\n",
      "weighted avg       0.99      0.99      0.99        80\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  1]\n",
      " [ 0 40]]\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Model Training with Class Weight Balancing ---\n",
    "# The class_weight='balanced' parameter is maintained to stabilize training \n",
    "# against feature skew, even with balanced data.\n",
    "\n",
    "print(\"\\n--- Training Logistic Regression with class_weight='balanced' ---\")\n",
    "# Using Logistic Regression as it's simple and effective for text classification\n",
    "model = LogisticRegression(class_weight='balanced', solver='liblinear', random_state=42) \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# --- 4. Evaluation ---\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"\\nClassification Report (Balanced Model):\")\n",
    "# Classification Report is crucial for checking the recall of both classes\n",
    "print(classification_report(y_test, y_pred, target_names=['Human', 'AI']))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "# Confusion Matrix shows the raw prediction counts\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Expected Outcome: The Recall for 'Human' should now be significantly higher than 0.03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ab58fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Model on New Examples ---\n",
      "\n",
      "--- Prediction for: 'dude what the heck was that about my boots' ---\n",
      "Predicted Class: Human\n",
      "Confidence (P(Human)): 0.5884\n",
      "\n",
      "--- Prediction for: 'Implementation of the recursive temporal parallax algorithm is contingent upon mitigating quantum entanglement flux.' ---\n",
      "Predicted Class: AI\n",
      "Confidence (P(AI)): 0.5276\n",
      "\n",
      "--- Prediction for: 'The collective melancholy of staplers is why I ran away like a bitch.' ---\n",
      "Predicted Class: Human\n",
      "Confidence (P(Human)): 0.5435\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Real-Time Inference (New Testing Block) ---\n",
    "# Testing the model on custom, unseen inputs\n",
    "\n",
    "def predict_text_class(text):\n",
    "    \"\"\"Predicts the class (Human or AI) for a single string of text.\"\"\"\n",
    "    # 1. Transform the new text using the *fitted* vectorizer\n",
    "    text_vec = vectorizer.transform([text])\n",
    "    \n",
    "    # 2. Get the prediction (0 or 1)\n",
    "    prediction = model.predict(text_vec)[0]\n",
    "    \n",
    "    # 3. Get the prediction probability\n",
    "    proba = model.predict_proba(text_vec)[0]\n",
    "    \n",
    "    class_name = 'AI' if prediction == 1 else 'Human'\n",
    "    confidence = proba[prediction]\n",
    "    \n",
    "    print(f\"\\n--- Prediction for: '{text}' ---\")\n",
    "    print(f\"Predicted Class: {class_name}\")\n",
    "    print(f\"Confidence (P({class_name})): {confidence:.4f}\")\n",
    "\n",
    "print(\"\\n--- Testing Model on New Examples ---\")\n",
    "\n",
    "# Example 1: Human-like text (informal, conversational)\n",
    "predict_text_class(\"dude what the heck was that about my boots\")\n",
    "\n",
    "# Example 2: AI-like text (formal, technical)\n",
    "predict_text_class(\"Implementation of the recursive temporal parallax algorithm is contingent upon mitigating quantum entanglement flux.\")\n",
    "\n",
    "# Example 3: Ambiguous text (should test the balance)\n",
    "predict_text_class(\"The collective melancholy of staplers is why I ran away like a bitch.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
